{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Cookie/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/Cookie/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/Cookie/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Sleeping\n",
    "import time as tm\n",
    "import random as rd\n",
    "\n",
    "## Tidying\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "## Scraping\n",
    "from bs4 import BeautifulSoup # https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "from selenium import webdriver # https://selenium-python.readthedocs.io/locating-elements.html\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "## NLP\n",
    "import nltk\n",
    "## Might need to download these\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "##\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "source": [
    "## Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beige Book URLs\n",
    "BB_2020_url = 'https://www.federalreserve.gov/monetarypolicy/beige-book-default.htm'\n",
    "BB_2019_1996_urls = 'https://www.federalreserve.gov/monetarypolicy/beige-book-archive.htm' # 2017-2020 has the same format\n",
    "\n",
    "# Configure Chrome Options for webdriver\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome('/Users/Cookie/node_modules/chromedriver/lib/chromedriver/chromedriver', \n",
    "                          chrome_options = chrome_options)"
   ]
  },
  {
   "source": [
    "## Class Creation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_engineering_1:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "\n",
    "    def year_link_create(self, yr:int):\n",
    "\n",
    "        link = ('https://www.federalreserve.gov/monetarypolicy/beigebook' + str(yr) + '.htm')\n",
    "\n",
    "        return link\n",
    "\n",
    "    def find_links(self, BB_url:str):\n",
    "\n",
    "        # print('Pulling ' + str(BB_url))\n",
    "\n",
    "        driver.get(BB_url)\n",
    "\n",
    "        # print('Loading page')\n",
    "\n",
    "        tm.sleep(rd.randint(2, 4))\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "        foundlinks = [str(l['href']) for l in soup.find_all(\"a\", href=re.compile(r\"[/]monetarypolicy[/]beigebook.*.htm\"))]\n",
    "\n",
    "        return foundlinks\n",
    "\n",
    "    def format_links(self, link: str):\n",
    "\n",
    "        if 'https://www.federalreserve.gov' not in link:\n",
    "\n",
    "            return ('https://www.federalreserve.gov' + link)\n",
    "\n",
    "        else:\n",
    "\n",
    "            return link\n",
    "\n",
    "    def simple_clean_corpus(self, corpus: str): \n",
    "\n",
    "                corpus = re.sub('\\n|<p>|</p>|<br/>|<strong>.*</strong>', '', corpus)\n",
    "\n",
    "                return corpus\n",
    "\n",
    "    def pull_corpora_17_20(self, links:list, date = [], overallEconomicActivity = [], employmentPrices = []):    \n",
    "\n",
    "        for n, link in enumerate(links):\n",
    "\n",
    "            driver.get(link)\n",
    "\n",
    "            # print('Loading link ' + str(n))\n",
    "\n",
    "            tm.sleep(rd.randint(1, 3))\n",
    "\n",
    "            reportSoup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "            if any(ext in link for ext in ['2017', '2018', '2019','2020']):\n",
    "\n",
    "                date.append(re.sub('Last Update:|\\n|\\\\s{2,}',\n",
    "                                   '',\n",
    "                                   reportSoup.find('div', {'class':'lastUpdate'}).text)) # Pulling date for the dataframe\n",
    "\n",
    "                textSoup = [str(t) for t in reportSoup.find_all('p')]\n",
    "\n",
    "                positionOEA = [i for i, s in enumerate(textSoup) if 'Overall Economic Activity' in s][0]\n",
    "                positionEW = [i for i, s in enumerate(textSoup) if 'Employment and Wages' in s][0]\n",
    "                positionP = [i for i, s in enumerate(textSoup) if 'Prices' in s][0]\n",
    "\n",
    "                corpusOverallEconomicActivity = ''.join(textSoup[positionOEA:positionEW])\n",
    "                corpusEmploymentPrices = ''.join(textSoup[positionEW:(positionP+1)])   \n",
    "\n",
    "                overallEconomicActivity.append(self.simple_clean_corpus(corpusOverallEconomicActivity))\n",
    "\n",
    "                employmentPrices.append(self.simple_clean_corpus(corpusEmploymentPrices))\n",
    "\n",
    "                print('Corpora from link ' + str(n) + ' cleaned and collected')\n",
    "\n",
    "        return date, overallEconomicActivity, employmentPrices        \n",
    "\n",
    "    def str_to_datetime(self, date: str):\n",
    "\n",
    "        date = re.sub('\\\\s', '', date)\n",
    "\n",
    "        date = datetime.datetime.strptime(date, '%B%d,%Y')\n",
    "\n",
    "        return date\n",
    "\n",
    "    def get_wordnet_pos(self, word: str):\n",
    "    \n",
    "        \"\"\"\n",
    "        Map POS tag to first character lemmatize() accepts, from:\n",
    "        https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#wordnetlemmatizerwithappropriatepostag\n",
    "        \"\"\"\n",
    "\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def remove_stop_lemma_words(self, col: str):\n",
    "    \n",
    "        '''\n",
    "        Removes stop words and retrieves lemmas of the words in the statements\n",
    "        I've chosen lemmas over stems because it is more nuanced and sophisticated. There's a good explanation here: \n",
    "        https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming\n",
    "        '''\n",
    "\n",
    "        stopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "        stopWords.add(' ')\n",
    "\n",
    "        col = col.lower() # all strings to lower\n",
    "\n",
    "        col = re.sub(r'\\\\s{2,}', ' ', col) # turn two or more spaces into 1 space\n",
    "\n",
    "        words = word_tokenize(col)\n",
    "\n",
    "        updated_words = []\n",
    "        for w in words:\n",
    "            \n",
    "            if w not in stopWords: # Only keep if word isn't a stop word\n",
    "                \n",
    "                w = wnl.lemmatize(w, self.get_wordnet_pos(w)) # get lemma of word\n",
    "                \n",
    "                updated_words.append(w)\n",
    "                \n",
    "        return updated_words                \n",
    "\n",
    "dataEngineering1 = data_engineering_1()    "
   ]
  },
  {
   "source": [
    "## Run Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collected Links\n",
      "Corpora from link 48 cleaned and collected\n",
      "Corpora from link 49 cleaned and collected\n",
      "Corpora from link 50 cleaned and collected\n",
      "Corpora from link 51 cleaned and collected\n",
      "Corpora from link 52 cleaned and collected\n",
      "Corpora from link 53 cleaned and collected\n",
      "Corpora from link 54 cleaned and collected\n",
      "Corpora from link 55 cleaned and collected\n",
      "Corpora from link 56 cleaned and collected\n",
      "Corpora from link 57 cleaned and collected\n",
      "Corpora from link 58 cleaned and collected\n",
      "Corpora from link 59 cleaned and collected\n",
      "Corpora from link 60 cleaned and collected\n",
      "Corpora from link 61 cleaned and collected\n",
      "Corpora from link 62 cleaned and collected\n",
      "Corpora from link 63 cleaned and collected\n",
      "Corpora from link 64 cleaned and collected\n",
      "Corpora from link 65 cleaned and collected\n",
      "Corpora from link 66 cleaned and collected\n",
      "Corpora from link 67 cleaned and collected\n",
      "Corpora from link 68 cleaned and collected\n",
      "Corpora from link 69 cleaned and collected\n",
      "Corpora from link 70 cleaned and collected\n",
      "Corpora from link 71 cleaned and collected\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        Date                            OverallEconomicActivity  \\\n",
       "0 2017-01-18  Reports from the twelve Federal Reserve Distri...   \n",
       "1 2017-03-01  Reports from all twelve Federal Reserve Distri...   \n",
       "\n",
       "                                    EmploymentPrices  \\\n",
       "0  Labor markets were reported to be tight or tig...   \n",
       "1  Labor markets remained tight in early 2017, wi...   \n",
       "\n",
       "                   OverallEconomicActivity_sentences  \\\n",
       "0  [Reports from the twelve Federal Reserve Distr...   \n",
       "1  [Reports from all twelve Federal Reserve Distr...   \n",
       "\n",
       "                          EmploymentPrices_sentences  \\\n",
       "0  [Labor markets were reported to be tight or ti...   \n",
       "1  [Labor markets remained tight in early 2017, w...   \n",
       "\n",
       "                       OverallEconomicActivity_words  \\\n",
       "0  [report, twelve, federal, reserve, district, i...   \n",
       "1  [report, twelve, federal, reserve, district, i...   \n",
       "\n",
       "                              EmploymentPrices_words  \n",
       "0  [labor, market, report, tight, tighten, period...  \n",
       "1  [labor, market, remain, tight, early, 2017, ,,...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>OverallEconomicActivity</th>\n      <th>EmploymentPrices</th>\n      <th>OverallEconomicActivity_sentences</th>\n      <th>EmploymentPrices_sentences</th>\n      <th>OverallEconomicActivity_words</th>\n      <th>EmploymentPrices_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-01-18</td>\n      <td>Reports from the twelve Federal Reserve Distri...</td>\n      <td>Labor markets were reported to be tight or tig...</td>\n      <td>[Reports from the twelve Federal Reserve Distr...</td>\n      <td>[Labor markets were reported to be tight or ti...</td>\n      <td>[report, twelve, federal, reserve, district, i...</td>\n      <td>[labor, market, report, tight, tighten, period...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-03-01</td>\n      <td>Reports from all twelve Federal Reserve Distri...</td>\n      <td>Labor markets remained tight in early 2017, wi...</td>\n      <td>[Reports from all twelve Federal Reserve Distr...</td>\n      <td>[Labor markets remained tight in early 2017, w...</td>\n      <td>[report, twelve, federal, reserve, district, i...</td>\n      <td>[labor, market, remain, tight, early, 2017, ,,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "years = range(1996,2020)\n",
    "\n",
    "BB_urls = list(map(dataEngineering1.year_link_create, years)) \n",
    "\n",
    "links = list(map(dataEngineering1.find_links, BB_urls))   \n",
    "\n",
    "links = [item for sublist in links for item in sublist] # unnests lists\n",
    "    \n",
    "updated_lists = list(map(dataEngineering1.format_links, links))\n",
    "\n",
    "print('Collected Links')\n",
    "\n",
    "date, overallEconomicActivity, employmentPrices = dataEngineering1.pull_corpora_17_20(updated_lists)\n",
    "\n",
    "beigeBookExtracts= pd.DataFrame({\"Date\":date,\n",
    "                                 \"OverallEconomicActivity\":overallEconomicActivity,\n",
    "                                 \"EmploymentPrices\":employmentPrices})\n",
    "\n",
    "beigeBookExtracts['Date'] = beigeBookExtracts['Date'].apply(lambda x: dataEngineering1.str_to_datetime(x))\n",
    "\n",
    "## Feature Creation ##\n",
    "\n",
    "## Adding sentences columns\n",
    "beigeBookExtracts['OverallEconomicActivity_sentences'] = beigeBookExtracts['OverallEconomicActivity'].apply(lambda x: sent_tokenize(x))\n",
    "\n",
    "beigeBookExtracts['EmploymentPrices_sentences'] = beigeBookExtracts['EmploymentPrices'].apply(lambda x: sent_tokenize(x))\n",
    "\n",
    "## Create Word Net Lemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "## Adding lemmatized word columns\n",
    "\n",
    "beigeBookExtracts['OverallEconomicActivity_words'] = beigeBookExtracts['OverallEconomicActivity'].apply(lambda x: dataEngineering1.remove_stop_lemma_words(x))\n",
    "\n",
    "beigeBookExtracts['EmploymentPrices_words'] = beigeBookExtracts['EmploymentPrices'].apply(lambda x: dataEngineering1.remove_stop_lemma_words(x))\n",
    "\n",
    "beigeBookExtracts.head(2)"
   ]
  },
  {
   "source": [
    "## Write CSV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beigeBookExtracts.to_csv('beigeBookExtracts.csv')"
   ]
  }
 ]
}