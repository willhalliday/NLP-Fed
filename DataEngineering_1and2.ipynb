{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sleeping\n",
    "import time as tm\n",
    "import random as rd\n",
    "\n",
    "## Tidying\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "## Scraping\n",
    "from bs4 import BeautifulSoup # https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "from selenium import webdriver # https://selenium-python.readthedocs.io/locating-elements.html\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "## NLP\n",
    "import nltk\n",
    "## Might need to download these\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "##\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beige Book URLs\n",
    "BB_2020_url = 'https://www.federalreserve.gov/monetarypolicy/beige-book-default.htm'\n",
    "BB_2019_1996_urls = 'https://www.federalreserve.gov/monetarypolicy/beige-book-archive.htm' # 2017-2020 has the same format\n",
    "\n",
    "# Configure Chrome Options for webdriver\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome('/Users/Cookie/node_modules/chromedriver/lib/chromedriver/chromedriver', \n",
    "                          chrome_options = chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_engineering_1:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "\n",
    "    def year_link_create(self, yr:int):\n",
    "\n",
    "        link = ('https://www.federalreserve.gov/monetarypolicy/beigebook' + str(yr) + '.htm')\n",
    "\n",
    "        return link\n",
    "\n",
    "    def find_links(self, BB_url:str):\n",
    "\n",
    "        print('Pulling ' + str(BB_url))\n",
    "\n",
    "        driver.get(BB_url)\n",
    "\n",
    "        print('Loading page')\n",
    "\n",
    "        tm.sleep(rd.randint(2, 4))\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "        foundlinks = [str(l['href']) for l in soup.find_all(\"a\", href=re.compile(r\"[/]monetarypolicy[/]beigebook.*.htm\"))]\n",
    "\n",
    "        return foundlinks\n",
    "\n",
    "    def format_links(self, link: str):\n",
    "\n",
    "        if 'https://www.federalreserve.gov' not in link:\n",
    "\n",
    "            return ('https://www.federalreserve.gov' + link)\n",
    "\n",
    "        else:\n",
    "\n",
    "            return link\n",
    "\n",
    "    def simple_clean_corpus(self, corpus: str): \n",
    "\n",
    "                corpus = re.sub('\\n|<p>|</p>|<br/>|<strong>.*</strong>', '', corpus)\n",
    "\n",
    "                return corpus\n",
    "\n",
    "    def pull_corpora_17_20(self, links:list, date = [], overallEconomicActivity = [], employmentPrices = []):    \n",
    "\n",
    "        for n, link in enumerate(links):\n",
    "\n",
    "            driver.get(link)\n",
    "\n",
    "            print('Loading link ' + str(n))\n",
    "\n",
    "            tm.sleep(rd.randint(1, 3))\n",
    "\n",
    "            reportSoup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "            if any(ext in link for ext in ['2017', '2018', '2019','2020']):\n",
    "\n",
    "                date.append(re.sub('Last Update:|\\n|\\\\s{2,}',\n",
    "                                   '',\n",
    "                                   reportSoup.find('div', {'class':'lastUpdate'}).text)) # Pulling date for the dataframe\n",
    "\n",
    "                textSoup = [str(t) for t in reportSoup.find_all('p')]\n",
    "\n",
    "                positionOEA = [i for i, s in enumerate(textSoup) if 'Overall Economic Activity' in s][0]\n",
    "                positionEW = [i for i, s in enumerate(textSoup) if 'Employment and Wages' in s][0]\n",
    "                positionP = [i for i, s in enumerate(textSoup) if 'Prices' in s][0]\n",
    "\n",
    "                corpusOverallEconomicActivity = ''.join(textSoup[positionOEA:positionEW])\n",
    "                corpusEmploymentPrices = ''.join(textSoup[positionEW:(positionP+1)])   \n",
    "\n",
    "                overallEconomicActivity.append(self.simple_clean_corpus(corpusOverallEconomicActivity))\n",
    "\n",
    "                employmentPrices.append(self.simple_clean_corpus(corpusEmploymentPrices))\n",
    "\n",
    "                print('Corpora from link ' + str(n) + ' cleaned and collected')\n",
    "\n",
    "        return date, overallEconomicActivity, employmentPrices        \n",
    "\n",
    "    def str_to_datetime(self, date: str):\n",
    "\n",
    "        date = re.sub('\\\\s', '', date)\n",
    "\n",
    "        date = datetime.datetime.strptime(date, '%B%d,%Y')\n",
    "\n",
    "        return date\n",
    "\n",
    "dataEngineering1 = data_engineering_1()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(1996,2020)\n",
    "\n",
    "BB_urls = list(map(dataEngineering1.year_link_create, years)) \n",
    "\n",
    "links = list(map(dataEngineering1.find_links, BB_urls))   \n",
    "\n",
    "links = [item for sublist in links for item in sublist] # unnests lists\n",
    "    \n",
    "updated_lists = list(map(dataEngineering1.format_links, links))\n",
    "\n",
    "print('Collected Links')\n",
    "\n",
    "date, overallEconomicActivity, employmentPrices = dataEngineering1.pull_corpora_17_20(updated_lists)\n",
    "\n",
    "beigeBookExtracts= pd.DataFrame({\"Date\":date,\n",
    "                                 \"OverallEconomicActivity\":overallEconomicActivity,\n",
    "                                 \"EmploymentPrices\":employmentPrices})\n",
    "\n",
    "beigeBookExtracts['Date'] = beigeBookExtracts['Date'].apply(lambda x: dataEngineering1.str_to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding sentences columns\n",
    "beigeBookExtracts['OverallEconomicActivity_sentences'] = beigeBookExtracts['OverallEconomicActivity'].apply(lambda x: sent_tokenize(x))\n",
    "beigeBookExtracts['EmploymentPrices_sentences'] = beigeBookExtracts['EmploymentPrices'].apply(lambda x: sent_tokenize(x))\n",
    "\n",
    "## Adding words columns\n",
    "\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "wnl = WordNetLemmatizer() # I've chosen this over stemmer because it is more nuanced and sophisticated. More detail here : https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming\n",
    "\n",
    "def remove_stop_lemma_words(col: str):\n",
    "    \n",
    "    col = re.sub(r'\\\\s{2,}', ' ', col)\n",
    "    \n",
    "    words = word_tokenize(col)\n",
    "    \n",
    "    updated_words = []\n",
    "    for w in words:\n",
    "        \n",
    "        if w not in [stopWords, '']: # Only keep if word isn't a stop word\n",
    "            \n",
    "            w = wnl.lemmatize(w) # get lemma of word\n",
    "            \n",
    "            updated_words.append(w)\n",
    "            \n",
    "    return updated_words        \n",
    "\n",
    "beigeBookExtracts['OverallEconomicActivity_words'] = beigeBookExtracts['OverallEconomicActivity'].apply(lambda x: remove_stop_lemma_words(x))\n",
    "\n",
    "beigeBookExtracts['EmploymentPrices_words'] = beigeBookExtracts['EmploymentPrices'].apply(lambda x: remove_stop_lemma_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beigeBookExtracts.to_csv('beigeBookExtracts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}